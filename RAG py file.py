# -*- coding: utf-8 -*-
"""langchain-multimodal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/alejandro-ao/47db0b8b9d00b10a96ab42dd59d90b86/langchain-multimodal.ipynb

# Multi-modal RAG with LangChain

## SetUp

Install the dependencies you need to run the notebook.
"""

# for linux
!apt-get install poppler-utils tesseract-ocr libmagic-dev

# Commented out IPython magic to ensure Python compatibility.
# %pip install -Uq "unstructured[all-docs]" pillow lxml pillow
# %pip install -Uq chromadb tiktoken
# %pip install -Uq langchain langchain-community langchain-openai langchain-groq
# %pip install -Uq python_dotenv

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-google-genai

import os

# keys for the services we will use

os.environ["GEMINI_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_TRACING_V2"] = "true"

"""## Extract the data

Extract the elements of the PDF that we will be able to use in the retrieval process. These elements can be: Text, Images, Tables, etc.

### Partition PDF tables, text, and images
"""

from unstructured.partition.pdf import partition_pdf

output_path = "/content/"
file_path = output_path + 'oai_gpt-oss_model_card.pdf'

# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking
chunks = partition_pdf(
    filename=file_path,
    infer_table_structure=True,            # extract tables
    strategy="hi_res",                     # mandatory to infer tables

    extract_image_block_types=["Image"],   # Add 'Table' to list to extract image of tables
    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64

    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage

    chunking_strategy="by_title",          # or 'basic'
    max_characters=10000,                  # defaults to 500
    combine_text_under_n_chars=2000,       # defaults to 0
    new_after_n_chars=6000,

    # extract_images_in_pdf=True,          # deprecated
)

# We get types of elements from the partition_pdf function
set([str(type(el)) for el in chunks])

len(chunks)

# Each CompositeElement containes a bunch of related elements.
# This makes it easy to use these elements together in a RAG pipeline.

chunks[0].metadata.orig_elements

# This is what an extracted image looks like.
# It contains the base64 representation only because we set the param extract_image_block_to_payload=True

elements = chunks[3].metadata.orig_elements
chunk_images = [el for el in elements if 'Image' in str(type(el))]
chunk_images[0].to_dict()

"""### Separate extracted elements into tables, text, and images"""

# separate tables from texts
tables = []
texts = []

for chunk in chunks:
    if "Table" in str(type(chunk)):
        tables.append(chunk)

    if "CompositeElement" in str(type((chunk))):
        texts.append(chunk)

# Get the images from the CompositeElement objects
def get_images_base64(chunks):
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):
            chunk_els = chunk.metadata.orig_elements
            for el in chunk_els:
                if "Image" in str(type(el)):
                    images_b64.append(el.metadata.image_base64)
    return images_b64

images = get_images_base64(chunks)

"""#### Check what the images look like"""

import base64
from IPython.display import Image, display

def display_base64_image(base64_code):
    # Decode the base64 string to binary
    image_data = base64.b64decode(base64_code)
    # Display the image
    display(Image(data=image_data))

display_base64_image(images[0])

"""## Summarize the data

Create a summary of each element extracted from the PDF. This summary will be vectorized and used in the retrieval process.

### Text and Table summaries

We don't need a multimodal model to generate the summaries of the tables and the text. I will use GEMINI model for the same.
"""

import getpass
import os

if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key:your-api-key")

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

prompt_text = """
You are an assistant tasked with summarizing tables and text.
Give a concise summary of the table or text.

Respond only with the summary, no additional comment.
Do not start your message by saying "Here is a summary" or anything like that.
Just give the summary as it is.

Table or text chunk: {element}
"""

prompt = ChatPromptTemplate.from_template(prompt_text)

# Replace with your Gemini API key
import os
os.environ["GOOGLE_API_KEY"] = "your-api-key"

model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.5
)

summarize_chain = (
    {"element": lambda x: x}
    | prompt
    | model
    | StrOutputParser()
)

# Summarize text
text_summaries = summarize_chain.batch(texts, {"max_concurrency": 3})

# Summarize tables
tables_html = [table.metadata.text_as_html for table in tables]
table_summaries = summarize_chain.batch(tables_html, {"max_concurrency": 3})

text_summaries

"""### Image summaries

We will use gemini-2.0-flash to produce the image summaries.
"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
import os

# Set API key
# os.environ["GOOGLE_API_KEY"] = "YOUR_API_KEY"

# Prompt template
prompt_template = """Describe the image in detail. For context,
the image is part of a research paper explaining the transformers architecture.
Be specific about graphs, such as bar plots."""

messages = [
    (
        "user",
        [
            {"type": "text", "text": prompt_template},
            {
                "type": "image_url",
                "image_url": {"url": "data:image/jpeg;base64,{image}"},
            },
        ],
    )
]

prompt = ChatPromptTemplate.from_messages(messages)

# Model
model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",  # Free, fast model for image understanding
    temperature=0.5
)

# Chain
chain = prompt | model | StrOutputParser()

# Run batch inference
image_summaries = chain.batch(images)

image_summaries

print(image_summaries[1])

"""## Load data and summaries to vectorstore

### Create the vectorstore
"""

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your-api-key"

import uuid
from langchain.vectorstores import Chroma
from langchain.storage import InMemoryStore
from langchain.schema.document import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.retrievers.multi_vector import MultiVectorRetriever

# Hugging Face embedding model (free)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# The vectorstore to use to index the child chunks
vectorstore = Chroma(
    collection_name="multi_modal_rag",
    embedding_function=embeddings
)

# The storage layer for the parent documents
store = InMemoryStore()
id_key = "doc_id"

# The retriever (empty to start)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
)

"""### Load the summaries and link the to the original data"""

# Add text chunks
doc_ids = [str(uuid.uuid4()) for _ in texts]
summary_texts_docs = [
    Document(page_content=summary, metadata={id_key: doc_ids[i]})
    for i, summary in enumerate(text_summaries)
]
retriever.vectorstore.add_documents(summary_texts_docs)
retriever.docstore.mset(list(zip(doc_ids, texts)))

# Add table chunks only if summaries are non-empty
table_ids = [str(uuid.uuid4()) for _ in tables]
summary_tables_docs = [
    Document(page_content=summary, metadata={id_key: table_ids[i]})
    for i, summary in enumerate(table_summaries)
    if summary and summary.strip()  # ✅ skip empty or None
]

if summary_tables_docs:  # ✅ only add if not empty
    retriever.vectorstore.add_documents(summary_tables_docs)
    retriever.docstore.mset(list(zip(table_ids, tables)))
else:
    print("⚠️ No valid table summaries to add.")

# Add image chunks
img_ids = [str(uuid.uuid4()) for _ in images]
summary_img_docs = [
    Document(page_content=summary, metadata={id_key: img_ids[i]})
    for i, summary in enumerate(image_summaries)
]
retriever.vectorstore.add_documents(summary_img_docs)
retriever.docstore.mset(list(zip(img_ids, images)))

"""### Check retrieval"""

# Retrieve
docs = retriever.invoke(
    "what is oss model about?"
)

for doc in docs:
    print(str(doc) + "\n\n" + "-" * 80)

"""## RAG pipeline"""

from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from base64 import b64decode
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# ---- 1. Make sure to set your Google API Key ----
# import os
# os.environ["GOOGLE_API_KEY"] = "YOUR_GOOGLE_API_KEY"

# ---- 2. Parse documents into images and text ----
def parse_docs(docs):
    """Split base64-encoded images and texts"""
    b64 = []
    text = []
    for doc in docs:
        try:
            b64decode(doc)  # If it's valid base64, treat as image
            b64.append(doc)
        except Exception:
            text.append(doc)
    return {"images": b64, "texts": text}

# ---- 3. Build the chat prompt for Gemini ----
def build_prompt(kwargs):
    docs_by_type = kwargs["context"]
    user_question = kwargs["question"]

    context_text = ""
    if len(docs_by_type["texts"]) > 0:
        for text_element in docs_by_type["texts"]:
            context_text += text_element.text

    # construct prompt with context (including images)
    prompt_template = f"""
    Answer the question based only on the following context, which can include text, tables, and the below image.
    Context: {context_text}
    Question: {user_question}
    """

    prompt_content = [{"type": "text", "text": prompt_template}]

    if len(docs_by_type["images"]) > 0:
        for image in docs_by_type["images"]:
            prompt_content.append(
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image}"},
                }
            )

    return ChatPromptTemplate.from_messages(
        [
            HumanMessage(content=prompt_content),
        ]
    )

# ---- 4. Create chain with Gemini (free model: gemini-1.5-flash) ----
gemini_chat = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",  # free & fast
    temperature=0,
)

chain = (
    {
        "context": retriever | RunnableLambda(parse_docs),
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(build_prompt)
    | gemini_chat
    | StrOutputParser()
)

# ---- 5. Chain with sources ----
chain_with_sources = {
    "context": retriever | RunnableLambda(parse_docs),
    "question": RunnablePassthrough(),
} | RunnablePassthrough().assign(
    response=(
        RunnableLambda(build_prompt)
        | gemini_chat
        | StrOutputParser()
    )
)

response = chain.invoke(
    "Summarize the models used in the paper. Explain in details what the model does."
)

print(response)

response = chain_with_sources.invoke(
    "-what are the use case of these models?"
)

print("Response:", response['response'])

print("\n\nContext:")
for text in response['context']['texts']:
    print(text.text)
    print("Page number: ", text.metadata.page_number)
    print("\n" + "-"*50 + "\n")
for image in response['context']['images']:
    display_base64_image(image)

"""## References

- [LangChain Inspiration](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev)
- [Multivector Storage](https://python.langchain.com/docs/how_to/multi_vector/)
"""